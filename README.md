# PySpark-for-Big-Data

Using pyspark to perform some basic operations like:
- create a schema for the data
- data analysis
- data visualization
- data preparation ( impute nulls and outliers)
- split data into train and test data
- create a pipeline model that contains:
   --VectorAssembler , StringIndexer , OneHotEncoder , LogisticRegression
- evaluate the model using ROC 

